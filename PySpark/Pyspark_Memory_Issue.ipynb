{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, StructType, StringType, LongType, IntegerType, StringType, FloatType, DataType, BooleanType, ArrayType, DoubleType\n",
    "\n",
    "# A Spark Session is how we interact with Spark SQL to create Dataframes\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# This will help catch some PySpark errors\n",
    "from py4j.protocol import Py4JJavaError\n",
    "\n",
    "from pyspark.sql.functions import * # isnan, when, count, col, monotonically_increasing_id, udf \n",
    "\n",
    "from pyspark.ml.feature import PCA, VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "import datetime \n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "spark = (SparkSession.builder.master('yarn')\n",
    ".appName('my_app').\n",
    ".config('spark.jars', 'gs://spark-lib/bigquery/spark-bigquery-latest.jar')\n",
    ".config('spark.scheduler.mode','FAIR')\n",
    ".config('spark.sql.cbo.enabled',True)\n",
    ".config('spark.sql.cbo.joinReorder.enabled',True)\n",
    ".config('spark.executer.instances',3)#8\n",
    ".config('spark.executer.cores',4)\n",
    ".config('spark.driver.cores',4)\n",
    ".config('spark.executer.memory','4g')\n",
    ".config('spark.driver.memory','4g')\n",
    ".config('spark.yarn.executor.memoryOverhead','4g')\n",
    ".config('spark.yarn.driver.memoryOverhead','4g')\n",
    ".config(\"sprak.yarn.queue\",'root.high')\n",
    ".getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'spark.app.id', u'application_1588908927341_0002'),\n",
       " (u'spark.eventLog.enabled', u'true'),\n",
       " (u'spark.dynamicAllocation.minExecutors', u'1'),\n",
       " (u'spark.driver.maxResultSize', u'123008m'),\n",
       " (u'spark.yarn.dist.jars',\n",
       "  u'gs://spark-lib/bigquery/spark-bigquery-latest.jar'),\n",
       " (u'spark.ui.proxyBase', u'/proxy/application_1588908927341_0002'),\n",
       " (u'spark.driver.appUIAddress',\n",
       "  u'http://cluster-tony1-m.c.umg-ds-jackalope.internal:4041'),\n",
       " (u'spark.yarn.am.memory', u'640m'),\n",
       " (u'spark.driver.port', u'33143'),\n",
       " (u'spark.executorEnv.PYTHONPATH',\n",
       "  u'{{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.10.7-src.zip'),\n",
       " (u'spark.driver.memory', u'500G'),\n",
       " (u'spark.app.name', u'my_app'),\n",
       " (u'spark.executor.instances', u'2'),\n",
       " (u'spark.jars', u'gs://spark-lib/bigquery/spark-bigquery-latest.jar'),\n",
       " (u'spark.serializer.objectStreamReset', u'100'),\n",
       " (u'spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_HOSTS',\n",
       "  u'cluster-tony1-m'),\n",
       " (u'spark.submit.deployMode', u'client'),\n",
       " (u'spark.ui.filters',\n",
       "  u'org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter'),\n",
       " (u'spark.executor.cores', u'8'),\n",
       " (u'spark.history.fs.logDirectory',\n",
       "  u'hdfs://cluster-tony1-m/user/spark/eventlog'),\n",
       " (u'spark.driver.host', u'cluster-tony1-m.c.umg-ds-jackalope.internal'),\n",
       " (u'spark.shuffle.service.enabled', u'true'),\n",
       " (u'spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_URI_BASES',\n",
       "  u'http://cluster-tony1-m:8088/proxy/application_1588908927341_0002'),\n",
       " (u'spark.scheduler.mode', u'FAIR'),\n",
       " (u'spark.hadoop.hive.execution.engine', u'mr'),\n",
       " (u'spark.yarn.jars', u'local:/usr/lib/spark/jars/*'),\n",
       " (u'spark.scheduler.minRegisteredResourcesRatio', u'0.0'),\n",
       " (u'spark.executor.id', u'driver'),\n",
       " (u'spark.repl.local.jars',\n",
       "  u'file:/tmp/spark-05243f8f-59b2-4413-a803-72e432a8970a/spark-bigquery-latest.jar'),\n",
       " (u'spark.dynamicAllocation.maxExecutors', u'10000'),\n",
       " (u'spark.executor.extraJavaOptions', u'-Xss10m -XX:MaxPermSize=128M'),\n",
       " (u'spark.driver.extraJavaOptions', u'-Xss10m -XX:MaxPermSize=512M'),\n",
       " (u'spark.yarn.historyServer.address', u'cluster-tony1-m:18080'),\n",
       " (u'spark.yarn.secondary.jars', u'spark-bigquery-latest.jar'),\n",
       " (u'spark.master', u'yarn'),\n",
       " (u'spark.rpc.message.maxSize', u'512'),\n",
       " (u'spark.rdd.compress', u'True'),\n",
       " (u'spark.eventLog.dir', u'hdfs://cluster-tony1-m/user/spark/eventlog'),\n",
       " (u'spark.executorEnv.OPENBLAS_NUM_THREADS', u'1'),\n",
       " (u'spark.yarn.isPython', u'true'),\n",
       " (u'spark.dynamicAllocation.enabled', u'true'),\n",
       " (u'spark.sql.parquet.cacheMetadata', u'false'),\n",
       " (u'spark.ui.showConsoleProgress', u'true'),\n",
       " (u'spark.executor.memory', u'200G'),\n",
       " (u'spark.sql.cbo.enabled', u'true')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = false)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- user_country_code: string (nullable = true)\n",
      " |-- target_variable: long (nullable = true)\n",
      " |-- r2_genre_desc: string (nullable = false)\n",
      " |-- has_canopy_genre: long (nullable = true)\n",
      " |-- apple_genre: string (nullable = false)\n",
      " |-- has_apple_genre: long (nullable = true)\n",
      " |-- final_track_genre: string (nullable = false)\n",
      " |-- days_since_release: long (nullable = true)\n",
      " |-- duration_ms: long (nullable = true)\n",
      " |-- num_markets: long (nullable = true)\n",
      " |-- key: long (nullable = false)\n",
      " |-- mode: long (nullable = false)\n",
      " |-- tempo: double (nullable = false)\n",
      " |-- time_signature: long (nullable = false)\n",
      " |-- stream_duration: long (nullable = true)\n",
      " |-- stream_country_code: string (nullable = true)\n",
      " |-- consumer_group_detail: string (nullable = true)\n",
      " |-- num_artists: long (nullable = false)\n",
      " |-- artist_begin_year: long (nullable = false)\n",
      " |-- artist_gender: string (nullable = false)\n",
      " |-- artist_geographic_area: string (nullable = false)\n",
      " |-- artist_country_code: string (nullable = false)\n",
      " |-- isrc_origin_country: string (nullable = false)\n",
      " |-- user_is_active_in_last_month_at_this_streamdatetime: long (nullable = true)\n",
      " |-- stream_local_hour: long (nullable = true)\n",
      " |-- stream_local_dayofweek: long (nullable = true)\n",
      " |-- days_since_highest_count_ever: long (nullable = false)\n",
      " |-- stream_count_last_30_days: long (nullable = false)\n",
      " |-- total_skips_last_30_days: long (nullable = false)\n",
      " |-- total_saves_last_30_days: long (nullable = false)\n",
      " |-- stream_count_30_days_after_release_on_spotify: long (nullable = false)\n",
      " |-- track_position_cur_chart_daily200: long (nullable = false)\n",
      " |-- chart_daily200_highest_position_ever: long (nullable = false)\n",
      " |-- chart_highest_rank_other_country_last_30_days: long (nullable = false)\n",
      " |-- in_chart_daily200_days_last_month: long (nullable = false)\n",
      " |-- user_stream_count_last_30days: long (nullable = false)\n",
      " |-- user_stream_duration_last_30days: long (nullable = false)\n",
      " |-- lean_back_stream_count_last_month: long (nullable = false)\n",
      " |-- lean_forward_stream_count_last_month: long (nullable = false)\n",
      " |-- user_stream_count_for_this_song_in_last_month: long (nullable = true)\n",
      " |-- has_searched_in_last_month: long (nullable = true)\n",
      " |-- num_of_playlist_changes_in_last_14_days: long (nullable = false)\n",
      " |-- num_streams_from_current_playlist_current_day: long (nullable = false)\n",
      " |-- total_historical_streams: long (nullable = false)\n",
      " |-- num_streams_from_current_playlist_last_month: long (nullable = false)\n",
      " |-- slope_7_days: double (nullable = false)\n",
      " |-- artist_genre: string (nullable = false)\n",
      " |-- other_artist_genre: string (nullable = false)\n",
      " |-- days_since_highest_count_3yrs_artist: long (nullable = false)\n",
      " |-- highest_artist_streaming_count_in_30days: long (nullable = false)\n",
      " |-- artist_total_number_of_in_chart_days_daily200_ever: long (nullable = false)\n",
      " |-- artist_popularity: long (nullable = false)\n",
      " |-- session_id: long (nullable = true)\n",
      " |-- artist_tier: string (nullable = false)\n",
      " |-- user_top_isrc_genre_in_last_month: string (nullable = false)\n",
      " |-- user_top_artist_genre_in_last_month: string (nullable = false)\n",
      " |-- num_listend_current_artist_in_last_month: long (nullable = false)\n",
      " |-- playlist_track_genre: string (nullable = false)\n",
      " |-- playlist_artist_genre: string (nullable = false)\n",
      " |-- most_common_playlist_track_genre_for_this_isrc: string (nullable = false)\n",
      " |-- most_common_playlist_artist_genre_for_this_isrc: string (nullable = false)\n",
      " |-- num_songs_listened_from_similar_artist_last_month: long (nullable = false)\n",
      " |-- num_of_repeat_play_for_isrc_in_last_30_days: long (nullable = false)\n",
      " |-- total_stream_count: long (nullable = false)\n",
      " |-- album_stream_count: long (nullable = false)\n",
      " |-- artist_stream_count: long (nullable = false)\n",
      " |-- collection_stream_count: long (nullable = false)\n",
      " |-- search_stream_count: long (nullable = false)\n",
      " |-- playlist_stream_count: long (nullable = false)\n",
      " |-- undeveloped_playlist_stream_count: long (nullable = false)\n",
      " |-- other_stream_count: long (nullable = false)\n",
      " |-- radio_stream_count: long (nullable = false)\n",
      " |-- play_queue_stream_count: long (nullable = false)\n",
      " |-- num_of_single_release_last_year: long (nullable = false)\n",
      " |-- num_of_single_release_2nd_last_year: long (nullable = false)\n",
      " |-- num_of_album_isrc_release_last_year: long (nullable = false)\n",
      " |-- num_of_album_isrc_release_2nd_last_year: long (nullable = false)\n",
      " |-- playlist_follower_count: long (nullable = false)\n",
      " |-- days_since_minimum_status_begin_date: long (nullable = false)\n",
      " |-- lag1_isrc_genre: string (nullable = false)\n",
      " |-- lag1_artist_genre: string (nullable = false)\n",
      " |-- days_since_playlist_creation_date: long (nullable = false)\n",
      " |-- content_owner: string (nullable = false)\n",
      " |-- is_user_listened_more_than_10days_in_last_month: long (nullable = true)\n",
      " |-- lag1_stream_source_same_as_current: long (nullable = false)\n",
      " |-- source_uri_type: string (nullable = true)\n",
      " |-- lag1_source_uri_same_as_current: long (nullable = false)\n",
      " |-- release_year: long (nullable = true)\n",
      " |-- release_month: long (nullable = true)\n",
      " |-- release_day: long (nullable = true)\n",
      " |-- has_duration_ms: long (nullable = true)\n",
      " |-- has_num_markets: long (nullable = true)\n",
      " |-- has_key: long (nullable = false)\n",
      " |-- has_mode: long (nullable = false)\n",
      " |-- has_time_signature: long (nullable = false)\n",
      " |-- is_umg_label: long (nullable = true)\n",
      " |-- user_age_new: long (nullable = true)\n",
      " |-- user_gender_new: string (nullable = true)\n",
      " |-- user_country_same_as_stream_country: long (nullable = false)\n",
      " |-- has_num_artists: long (nullable = false)\n",
      " |-- has_begin_year: long (nullable = false)\n",
      " |-- has_artist_gender: long (nullable = false)\n",
      " |-- has_geographic_area: long (nullable = false)\n",
      " |-- has_country_code: long (nullable = false)\n",
      " |-- has_isrc_origin_country: long (nullable = false)\n",
      " |-- has_user_minutes_since_last_stream: long (nullable = false)\n",
      " |-- user_minutes_since_last_stream: long (nullable = true)\n",
      " |-- artist_minimum_status_begin_year: long (nullable = true)\n",
      " |-- artist_minimum_status_begin_month: long (nullable = true)\n",
      " |-- artist_minimum_status_begin_day: long (nullable = true)\n",
      " |-- has_playlist_id: long (nullable = false)\n",
      " |-- has_playlist_creation_date: long (nullable = false)\n",
      " |-- playlist_creation_year: long (nullable = false)\n",
      " |-- playlist_creation_month: long (nullable = false)\n",
      " |-- playlist_creation_day: long (nullable = false)\n",
      " |-- has_track_position_cur_chart_daily200: long (nullable = false)\n",
      " |-- has_chart_daily200_highest_position_ever: long (nullable = false)\n",
      " |-- has_chart_highest_rank_other_country_last_30_days: long (nullable = false)\n",
      " |-- has_in_chart_daily200_days_last_month: long (nullable = false)\n",
      " |-- has_top_utc_hour: long (nullable = false)\n",
      " |-- top_utc_hour: long (nullable = false)\n",
      " |-- top_local_hour_in_one_month: long (nullable = false)\n",
      " |-- has_another_artist_id: long (nullable = false)\n",
      " |-- has_playlist_changes: long (nullable = false)\n",
      " |-- has_days_since_highest_count_3yrs_artist: long (nullable = false)\n",
      " |-- has_highest_artist_streaming_count_in_30days: long (nullable = false)\n",
      " |-- has_artist_total_number_of_in_chart_days_daily200_ever: long (nullable = false)\n",
      " |-- has_artist_popularity: long (nullable = false)\n",
      " |-- lag1_isrc_genre_same_as_current: long (nullable = false)\n",
      " |-- lag1_artist_genre_same_as_current: long (nullable = false)\n",
      " |-- user_top_isrc_genre_in_last_month_same_as_track_genre: long (nullable = false)\n",
      " |-- user_top_artist_genre_in_last_month_same_as_artist_genre: long (nullable = false)\n",
      " |-- has_num_listend_current_artist_in_last_month: long (nullable = false)\n",
      " |-- has_num_songs_listened_from_similar_artist_last_month: long (nullable = false)\n",
      " |-- pca0: double (nullable = true)\n",
      " |-- pca1: double (nullable = true)\n",
      " |-- pca2: double (nullable = true)\n",
      " |-- shuffle_play_flag: string (nullable = true)\n",
      " |-- repeat_play_flag: string (nullable = true)\n",
      " |-- completed_stream_flag: string (nullable = true)\n",
      " |-- cached_play_flag: string (nullable = true)\n",
      " |-- artist_cluster: string (nullable = true)\n",
      " |-- label_company: string (nullable = false)\n",
      " |-- user_region_code: string (nullable = false)\n",
      "\n",
      "None\n",
      "1018.16358709\n"
     ]
    }
   ],
   "source": [
    "# print(table_df.printSchema()) The schema for the data frame I am using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30976533\n",
      "146\n"
     ]
    }
   ],
   "source": [
    "# print(table_df.count()) \n",
    "# print(len(table_df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.657861948013\n"
     ]
    }
   ],
   "source": [
    "# Train test split\n",
    "st = time.time()\n",
    "train, test = table_df.randomSplit([0.8, 0.2], seed = 2018)\n",
    "print(time.time() - st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from pyspark.ml.classification import RandomForestClassificationModel\n",
    "import time \n",
    "\n",
    "\n",
    "from itertools import chain\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import auc\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This was what I used to train the model\n",
    "# rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\",maxDepth=27, numTrees=100) \n",
    "\n",
    "# st = time.time()\n",
    "# rfModel_27 = rf.fit(train)\n",
    "# print(time.time() - st)\n",
    "\n",
    "# ## get predictions on test set \n",
    "# st = time.time()\n",
    "# predictions_27 = rfModel_27.transform(test) \n",
    "# print(time.time() - st) \n",
    "\n",
    "#rfModel_27.save('gs://some_bucket/rfModel_27')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pretrained random forest model \n",
    "rfModel_27 = RandomForestClassificationModel.load('gs://some_bucket/rfModel_27')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_27 = rfModel_27.transform(test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_27.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o8860.javaToPython.\n: java.lang.OutOfMemoryError: Requested array size exceeds VM limit\n\tat java.util.Arrays.copyOf(Arrays.java:3236)\n\tat java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:118)\n\tat java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)\n\tat java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153)\n\tat org.apache.spark.util.ByteBufferOutputStream.write(ByteBufferOutputStream.scala:41)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1877)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.setBlockDataMode(ObjectOutputStream.java:1786)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1189)\n\tat java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)\n\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:43)\n\tat org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)\n\tat org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:342)\n\tat org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:335)\n\tat org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:159)\n\tat org.apache.spark.SparkContext.clean(SparkContext.scala:2299)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1.apply(RDD.scala:850)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1.apply(RDD.scala:849)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.mapPartitionsWithIndex(RDD.scala:849)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:625)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n\tat org.apache.spark.sql.Dataset.javaToPython(Dataset.scala:3197)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-037ab93ad990>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# get rdd of predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mscoreAndLabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictions_27\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'probability'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'probability'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/dataframe.pyc\u001b[0m in \u001b[0;36mrdd\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \"\"\"\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lazy_rdd\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0mjrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjavaToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lazy_rdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lazy_rdd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/anaconda/lib/python2.7/site-packages/py4j/java_gateway.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/anaconda/lib/python2.7/site-packages/py4j/protocol.pyc\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o8860.javaToPython.\n: java.lang.OutOfMemoryError: Requested array size exceeds VM limit\n\tat java.util.Arrays.copyOf(Arrays.java:3236)\n\tat java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:118)\n\tat java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)\n\tat java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153)\n\tat org.apache.spark.util.ByteBufferOutputStream.write(ByteBufferOutputStream.scala:41)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1877)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.setBlockDataMode(ObjectOutputStream.java:1786)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1189)\n\tat java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)\n\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:43)\n\tat org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)\n\tat org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:342)\n\tat org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:335)\n\tat org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:159)\n\tat org.apache.spark.SparkContext.clean(SparkContext.scala:2299)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1.apply(RDD.scala:850)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1.apply(RDD.scala:849)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.mapPartitionsWithIndex(RDD.scala:849)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:625)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n\tat org.apache.spark.sql.Dataset.javaToPython(Dataset.scala:3197)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n"
     ]
    }
   ],
   "source": [
    "# get rdd of predictions\n",
    "scoreAndLabels = predictions_27.select('label','probability').rdd.map(lambda row: (float(row['probability'][1]), float(row['label'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
